{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.preprocessors import ImageReader\n",
    "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding\n",
    "from mltu.augmentors import RandomBrightness, RandomErodeDilate, RandomSharpen\n",
    "from mltu.annotations.images import CVImage\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
    "from mltu.tensorflow.metrics import CERMetric, WERMetric\n",
    "from model import train_model\n",
    "from configs import ModelConfigs\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "try:\n",
    "    [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sentences_txt_path = \"Dataset/metadata/sentences.txt\"\n",
    "sentences_folder_path = \"Dataset/dataset\"\n",
    "\n",
    "dataset, vocab, max_len = [], set(), 0\n",
    "words = open(sentences_txt_path, \"r\").readlines()\n",
    "\n",
    "for line in tqdm(words):\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "\n",
    "    line_split = line.split(\" \")\n",
    "    if line_split[2] == \"err\":\n",
    "        continue\n",
    "\n",
    "    file_name = line_split[0] + \".png\"\n",
    "    label = line_split[-1].rstrip(\"\\n\")\n",
    "    label = label.replace(\"|\", \" \")\n",
    "\n",
    "    rel_path = os.path.join(sentences_folder_path, file_name)\n",
    "    if not os.path.exists(rel_path):\n",
    "        print(f\"File not found: {rel_path}\")\n",
    "        continue\n",
    "\n",
    "    dataset.append([rel_path, label])\n",
    "    vocab.update(list(label))\n",
    "    max_len = max(max_len, len(label))\n",
    "\n",
    "\n",
    "configs = ModelConfigs()\n",
    "configs.vocab = \"\".join(vocab)\n",
    "configs.max_text_length = max_len\n",
    "configs.save()\n",
    "\n",
    "data_provider = DataProvider(\n",
    "    dataset=dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height, keep_aspect_ratio=True),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "train_data_provider, val_data_provider = data_provider.split(split=0.9)\n",
    "\n",
    "train_data_provider.augmentors = [\n",
    "    RandomBrightness(),\n",
    "    RandomErodeDilate(),\n",
    "    RandomSharpen(),\n",
    "]\n",
    "\n",
    "def create_tf_dataset(data_provider):\n",
    "    def generator():\n",
    "        for data, label in data_provider:\n",
    "            \n",
    "            for i in range(len(data)):\n",
    "                yield data[i], label[i]  \n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(configs.height, configs.width, 3), dtype=tf.float32),  \n",
    "            tf.TensorSpec(shape=(configs.max_text_length,), dtype=tf.int32)  \n",
    "        )\n",
    "    )\n",
    "    dataset = dataset.batch(configs.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_tf_dataset(train_data_provider)\n",
    "val_dataset = create_tf_dataset(val_data_provider)\n",
    "\n",
    "model = train_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab),\n",
    ")\n",
    "\n",
    "\n",
    "class Float32CastingLayer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "\n",
    "outputs = Float32CastingLayer()(model.output)\n",
    "model = tf.keras.Model(inputs=model.input, outputs=outputs)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "model.summary(line_length=110)\n",
    "\n",
    "\n",
    "earlystopper = EarlyStopping(monitor=\"val_CER\", patience=20, verbose=1, mode=\"min\")\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "train_logger = TrainLogger(configs.model_path)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(configs.model_path, \"logs\"), update_freq=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.9, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\")\n",
    "model_to_onnx = Model2onnx(f\"{configs.model_path}/model.onnx\")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=[earlystopper, checkpoint, train_logger, reduce_lr, tensorboard, model_to_onnx],\n",
    ")\n",
    "\n",
    "\n",
    "train_data_provider.to_csv(os.path.join(configs.model_path, \"train.csv\"))\n",
    "val_data_provider.to_csv(os.path.join(configs.model_path, \"val.csv\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Try",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
